<p align="center">
<h2 align="center"> Scene Prior Filtering for Depth Map Super-Resolution </h2>

<p align="center"><a href="https://scholar.google.com/citations?user=VogTuQkAAAAJ&hl=zh-CN">Zhengxue Wang</a><sup>1</sup>, 
<a href="https://yanzq95.github.io/">Zhiqiang Yan</a><sup>1</sup>, 
<a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a><sup>2</sup>, 
<a href="https://jspan.github.io/">Jinshan Pan</a><sup>1</sup>,
<a href="https://guangweigao.github.io/">Guangwei Gao</a><sup>3</sup>,
<a href="https://tyshiwo.github.io/">Ying Tai</a><sup>4</sup>,
  <a href="https://scholar.google.com/citations?user=6CIDtZQAAAAJ&hl=zh-CN">Jian Yang</a><sup>1</sup>  <!--&Dagger;-->
</p>

<p align="center">
  <!--<sup>&dagger;</sup>equal contribution&nbsp;&nbsp;&nbsp;-->
   <!--<sup>&Dagger;</sup>corresponding author&nbsp;&nbsp;&nbsp;<br>-->
  <sup>1</sup>Nanjing University of Science and Technology&nbsp;&nbsp;&nbsp;
  <sup>2</sup>University of California at Merced&nbsp;&nbsp;&nbsp;<br>
  <sup>3</sup>Nanjing University of Posts and Telecommunications&nbsp;&nbsp;&nbsp;
  <sup>4</sup>Nanjing University&nbsp;&nbsp;&nbsp;
</p>

<p align="center">
[<a href="https://arxiv.org/pdf/2402.13876.pdf"><strong>Paper</strong></a>]
[<a href="https://yanzq95.github.io/projectpage/SPFNet/index.html"><strong>Project Page</strong></a>]
</p>

<p align="center">
<img src="Figs/Overview.png"/>
</p>

Overview of SPFNet (left) and its distilled variant SPFNet-D (right). For SPFNet, large-scale models are first employed to generate the normal prior $\boldsymbol I_{n}$ and semantic prior $\boldsymbol I_{s}$ from RGB $\boldsymbol I_{r}$. Then, $\boldsymbol I_{n}$ and $\boldsymbol I_{s}$ are fed together with $\boldsymbol I_{r}$ into multiple iteratively executed APP-OPE modules (APP and OPE are connected in series) to transfer scene prior knowledge into depth features, thereby reconstructing HR depth $\boldsymbol D_{hr}$. For SPFNet-D, the encoded features of normal and semantic models are separately extracted, yielding $\boldsymbol F_{n}^{e}$ and $\boldsymbol F_{s}^{e}$. Additionally, a prior distillation regularization (consisting of normal term $L_{nor}$ and semantic term $L_{sem}$) is introduced to distill normal and semantic features into RGB features $\boldsymbol F_{r}^{n}$ and $\boldsymbol F_{r}^{s}$, effectively reducing the computational overhead of large-scale models during inference. The distilled $\boldsymbol F_{r}^{n}$ and $\boldsymbol F_{r}^{s}$ replace the normal and semantic feature flows as input to the depth branch, whose architecture is kept identical to that of the original SPFNet.

<p align="center">
<img src="Figs/APP.png"/>
</p>

Scheme of (a) All-in-one Prior Propagation (APP), and (b) histogram comparison of scene prior features. Features $\boldsymbol F_{dn}^{i-1}$, $\boldsymbol F_{dr}^{i-1}$, and $\boldsymbol F_{ds}^{i-1}$ are downsampled from $\boldsymbol F_{n}^{i-1}$, $\boldsymbol F_{r}^{i-1}$, and $\boldsymbol F_{s}^{i-1}$, matching the size of depth features $\boldsymbol F_{d}^{i-1}$.

<p align="center">
<img src="Figs/OPE.png"/>
</p>

Scheme of (a) One-to-one Prior Embedding (OPE), and (b) gradient histogram of filter kernels in the texture area (green box). The surface normal, semantic, and RGB kernels are generated by our Mutual Guided Filtering (MGF).




## Dependencies

```bash
Python==3.11.5
PyTorch==2.1.0
numpy==1.23.5 
torchvision==0.16.0
scipy==1.11.3
Pillow==10.0.1
tqdm==4.65.0
scikit-image==0.21.0
```

## Datasets
All Datasets can be found <a href="https://drive.google.com/file/d/1Sx7RUkV3pXoAzFxAsGG5lEx74sx6Qdvc/view?usp=sharing">here</a>.

## Models
All pretrained models can be found <a href="https://drive.google.com/drive/folders/1njJPTCmfe4YhLN-95awFrN2RhW5ThRdh?usp=sharing">here</a>.

## Training

### SPFNet
```
Train on synthetic NYU-v2
# x4 DSR
> python train.py --scale 4 --num_feats 42
# x8 DSR
> python train.py --scale 8 --num_feats 42
# x16 DSR
> python train.py --scale 16 --num_feats 42
Train on real-world RGB-D-D
> python train.py --scale 4 --num_feats 20 

```

### SPFNet-T
```
Train on synthetic NYU-v2
# x4 DSR
> python train.py --scale 4 --num_feats 6 --tiny_model
# x8 DSR
> python train.py --scale 8 --num_feats 6 --tiny_model
# x16 DSR
> python train.py --scale 16 --num_feats 6 --tiny_model
Train on real-world RGB-D-D
> python train.py --scale 4 --num_feats 6 --tiny_model
```

## Testing

### SPFNet
```
## Test on synthetic datasets
### x4 DSR
> python test.py --scale 4 --num_feats 42
### x8 DSR
> python test.py --scale 8 --num_feats 42
### x16 DSR
> python test.py --scale 16 --num_feats 42
## Test on real-world RGB-D-D
> python test.py --scale 4 --num_feats 20 --downsample real
```

### SPFNet-T
```
## Test on synthetic datasets
### x4 DSR
> python test.py --scale 4 --num_feats 6 --tiny_model
### x8 DSR
> python test.py --scale 8 --num_feats 6 --tiny_model
### x16 DSR
> python test.py --scale 16 --num_feats 6 --tiny_model
## Test on real-world RGB-D-D
> python test.py --scale 4 --num_feats 6 --downsample real --tiny_model
```

## Experiments

### Quantitative comparison

<p align="center">
<img src="Figs/Params_Time.png"/>
</p>

### Visual comparison

<b>Train & test on real-world RGB-D-D: <b/>
<p align="center">
<img src="Figs/Result_RGBDD_Real.png"/>
</p>

<b>Train & test on synthetic NYU-v2 (x16): <b/>
<p align="center">
<img src="Figs/Result_NYU.png"/>
</p>

## Acknowledgements

We thank [Xinni Jiang](https://jiangxinni.github.io/) for her invaluable assistance.

We thank these repos sharing their codes: [DKN](https://github.com/cvlab-yonsei/dkn) and [SUFT](https://github.com/ShiWuxuan/SUFT).


## Citation

```
@article{wang2024scene,
  title={Scene Prior Filtering for Depth Map Super-Resolution},
  author={Wang, Zhengxue and Yan, Zhiqiang and Yang, Ming-Hsuan and Pan, Jinshan and Yang, Jian and Tai, Ying and Gao, Guangwei},
  journal={arXiv preprint arXiv:2402.13876},
  year={2024}
}
```

